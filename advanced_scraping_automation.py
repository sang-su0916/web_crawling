#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ìë™í™” ë„êµ¬
Advanced Web Scraping Automation Tool

ğŸ•·ï¸ 24ì‹œê°„ ìë™í™” ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ
- ë‹¤ì–‘í•œ ì›¹ì‚¬ì´íŠ¸ ë™ì‹œ ëª¨ë‹ˆí„°ë§
- ìë™ Excel ë‚´ë³´ë‚´ê¸° ë° SQLite ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
- ì´ë©”ì¼/ì›¹í›… ì•Œë¦¼ ì‹œìŠ¤í…œ
- ê°•ë ¥í•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
- ë¹„ê°œë°œìë„ ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥

Author: Created with Claude Code
Version: 1.0.0
License: MIT
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
import json
import logging
import schedule
import time
import smtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import warnings
warnings.filterwarnings('ignore')

class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, db_path: str = "scraping_data.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scraping_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                website_name TEXT NOT NULL,
                url TEXT NOT NULL,
                title TEXT,
                content TEXT,
                price TEXT,
                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                hash_value TEXT UNIQUE
            )
        ''')
        
        # ë³€ê²½ ë¡œê·¸ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS change_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                website_name TEXT NOT NULL,
                change_type TEXT NOT NULL,
                old_value TEXT,
                new_value TEXT,
                changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def insert_data(self, data: Dict[str, Any]) -> bool:
        """ë°ì´í„° ì‚½ì…"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR IGNORE INTO scraping_results 
                (website_name, url, title, content, price, hash_value)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                data['website_name'],
                data['url'],
                data.get('title', ''),
                data.get('content', ''),
                data.get('price', ''),
                data.get('hash_value', '')
            ))
            
            conn.commit()
            success = cursor.rowcount > 0
            conn.close()
            return success
            
        except Exception as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì… ì˜¤ë¥˜: {e}")
            return False
    
    def get_recent_data(self, hours: int = 24) -> List[Dict]:
        """ìµœê·¼ ë°ì´í„° ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM scraping_results 
                WHERE scraped_at > datetime('now', '-{} hours')
                ORDER BY scraped_at DESC
            '''.format(hours))
            
            columns = [description[0] for description in cursor.description]
            results = [dict(zip(columns, row)) for row in cursor.fetchall()]
            
            conn.close()
            return results
            
        except Exception as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

class NotificationManager:
    """ì•Œë¦¼ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.email_config = config.get('email', {})
        self.webhook_config = config.get('webhook', {})
    
    def send_email(self, subject: str, body: str) -> bool:
        """ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡"""
        if not self.email_config.get('enabled', False):
            return True
            
        try:
            msg = MimeMultipart()
            msg['From'] = self.email_config['smtp_user']
            msg['To'] = self.email_config['to_email']
            msg['Subject'] = subject
            
            msg.attach(MimeText(body, 'html'))
            
            server = smtplib.SMTP(self.email_config['smtp_server'], self.email_config['smtp_port'])
            server.starttls()
            server.login(self.email_config['smtp_user'], self.email_config['smtp_password'])
            server.send_message(msg)
            server.quit()
            
            logging.info(f"ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ: {subject}")
            return True
            
        except Exception as e:
            logging.error(f"ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False
    
    def send_webhook(self, data: Dict[str, Any]) -> bool:
        """ì›¹í›… ì•Œë¦¼ ì „ì†¡"""
        if not self.webhook_config.get('enabled', False):
            return True
            
        try:
            response = requests.post(
                self.webhook_config['url'],
                json=data,
                timeout=10
            )
            response.raise_for_status()
            
            logging.info("ì›¹í›… ì „ì†¡ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logging.error(f"ì›¹í›… ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False

class AdvancedWebScraper:
    """ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "scraper_config.json"):
        self.setup_logging()
        self.load_config(config_path)
        self.db_manager = DatabaseManager()
        self.notification_manager = NotificationManager(self.config.get('notifications', {}))
        self.session = requests.Session()
        self.setup_session()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraping_automation.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
    def load_config(self, config_path: str):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            else:
                # ê¸°ë³¸ ì„¤ì • ìƒì„±
                self.config = self.create_default_config()
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(self.config, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logging.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.config = self.create_default_config()
    
    def create_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        return {
            "websites": [
                {
                    "name": "example_news",
                    "url": "https://example.com/news",
                    "type": "news",
                    "selectors": {
                        "title": ".news-title",
                        "content": ".news-content"
                    },
                    "schedule": "*/30",
                    "enabled": False,
                    "use_selenium": False
                }
            ],
            "general_settings": {
                "user_agent": "Advanced Web Scraper 1.0",
                "timeout": 10,
                "max_retries": 3,
                "delay_between_requests": 1,
                "max_workers": 5
            },
            "notifications": {
                "email": {
                    "enabled": False,
                    "smtp_server": "smtp.gmail.com",
                    "smtp_port": 587,
                    "smtp_user": "your-email@gmail.com",
                    "smtp_password": "your-app-password",
                    "to_email": "recipient@example.com"
                },
                "webhook": {
                    "enabled": False,
                    "url": "https://hooks.example.com/webhook"
                }
            },
            "export_settings": {
                "excel_export": True,
                "export_schedule": "daily",
                "max_records_per_file": 10000
            }
        }
    
    def setup_session(self):
        """HTTP ì„¸ì…˜ ì„¤ì •"""
        headers = {
            'User-Agent': self.config['general_settings']['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session.headers.update(headers)
    
    def get_selenium_driver(self) -> webdriver.Chrome:
        """Selenium ë“œë¼ì´ë²„ ìƒì„±"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument(f'--user-agent={self.config["general_settings"]["user_agent"]}')
        
        try:
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(self.config['general_settings']['timeout'])
            return driver
        except Exception as e:
            logging.error(f"Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def scrape_website(self, website_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
        if not website_config.get('enabled', True):
            return None
            
        url = website_config['url']
        logging.info(f"ìŠ¤í¬ë˜í•‘ ì‹œì‘: {website_config['name']} - {url}")
        
        try:
            if website_config.get('use_selenium', False):
                return self.scrape_with_selenium(website_config)
            else:
                return self.scrape_with_requests(website_config)
                
        except Exception as e:
            logging.error(f"ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {website_config['name']}: {e}")
            return None
    
    def scrape_with_requests(self, website_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """requestsë¥¼ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
        url = website_config['url']
        max_retries = self.config['general_settings']['max_retries']
        
        for attempt in range(max_retries):
            try:
                response = self.session.get(
                    url, 
                    timeout=self.config['general_settings']['timeout']
                )
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                return self.extract_data(soup, website_config)
                
            except requests.RequestException as e:
                logging.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    raise
    
    def scrape_with_selenium(self, website_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Seleniumì„ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
        driver = None
        try:
            driver = self.get_selenium_driver()
            driver.get(website_config['url'])
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (JavaScript ë¡œë”©)
            time.sleep(3)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            return self.extract_data(soup, website_config)
            
        except (TimeoutException, WebDriverException) as e:
            logging.error(f"Selenium ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None
        finally:
            if driver:
                driver.quit()
    
    def extract_data(self, soup: BeautifulSoup, website_config: Dict[str, Any]) -> Dict[str, Any]:
        """HTMLì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        selectors = website_config['selectors']
        data = {
            'website_name': website_config['name'],
            'url': website_config['url'],
            'scraped_at': datetime.now().isoformat()
        }
        
        for field, selector in selectors.items():
            try:
                element = soup.select_one(selector)
                if element:
                    data[field] = element.get_text(strip=True)
                else:
                    data[field] = ""
            except Exception as e:
                logging.warning(f"ìš”ì†Œ ì¶”ì¶œ ì‹¤íŒ¨ {field}: {e}")
                data[field] = ""
        
        # ë°ì´í„° í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê²€ì‚¬ìš©)
        content_hash = str(hash(f"{data.get('title', '')}{data.get('content', '')}{data.get('price', '')}"))
        data['hash_value'] = content_hash
        
        return data
    
    def process_scraped_data(self, data: Dict[str, Any]) -> bool:
        """ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„° ì²˜ë¦¬"""
        if not data:
            return False
            
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        is_new = self.db_manager.insert_data(data)
        
        if is_new:
            logging.info(f"ìƒˆë¡œìš´ ë°ì´í„° ë°œê²¬: {data['website_name']}")
            
            # ì•Œë¦¼ ì „ì†¡
            self.send_change_notification(data)
            
        return is_new
    
    def send_change_notification(self, data: Dict[str, Any]):
        """ë³€ê²½ ì•Œë¦¼ ì „ì†¡"""
        subject = f"ğŸ”” ìƒˆë¡œìš´ ë°ì´í„° ë°œê²¬: {data['website_name']}"
        
        html_body = f"""
        <html>
        <body>
            <h2>ì›¹ ìŠ¤í¬ë˜í•‘ ì•Œë¦¼</h2>
            <p><strong>ì›¹ì‚¬ì´íŠ¸:</strong> {data['website_name']}</p>
            <p><strong>URL:</strong> <a href="{data['url']}">{data['url']}</a></p>
            <p><strong>ì œëª©:</strong> {data.get('title', 'N/A')}</p>
            <p><strong>ë‚´ìš©:</strong> {data.get('content', 'N/A')[:200]}...</p>
            <p><strong>ê°€ê²©:</strong> {data.get('price', 'N/A')}</p>
            <p><strong>ìˆ˜ì§‘ ì‹œê°„:</strong> {data['scraped_at']}</p>
            
            <hr>
            <p><small>Advanced Web Scraping Automation Tool</small></p>
        </body>
        </html>
        """
        
        # ì´ë©”ì¼ ì „ì†¡
        self.notification_manager.send_email(subject, html_body)
        
        # ì›¹í›… ì „ì†¡
        webhook_data = {
            'type': 'new_data',
            'website': data['website_name'],
            'title': data.get('title', ''),
            'url': data['url'],
            'timestamp': data['scraped_at']
        }
        self.notification_manager.send_webhook(webhook_data)
    
    def run_single_scrape(self):
        """ë‹¨ì¼ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        logging.info("ìŠ¤í¬ë˜í•‘ ì‘ì—… ì‹œì‘")
        
        results = []
        for website_config in self.config['websites']:
            if website_config.get('enabled', True):
                data = self.scrape_website(website_config)
                if data:
                    self.process_scraped_data(data)
                    results.append(data)
                    
                # ìš”ì²­ ê°„ ì§€ì—°
                time.sleep(self.config['general_settings']['delay_between_requests'])
        
        logging.info(f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì™„ë£Œ: {len(results)}ê°œ ì›¹ì‚¬ì´íŠ¸ ì²˜ë¦¬")
        
        # ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± í™•ì¸
        if self.config['export_settings']['excel_export']:
            self.maybe_generate_daily_report()
    
    def maybe_generate_daily_report(self):
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì—¬ë¶€ í™•ì¸"""
        now = datetime.now()
        
        # ë§¤ì¼ ìì •ì— ë¦¬í¬íŠ¸ ìƒì„±
        if now.hour == 0 and now.minute < 5:
            self.generate_excel_report()
    
    def generate_excel_report(self):
        """Excel ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            # ìµœê·¼ 24ì‹œê°„ ë°ì´í„° ì¡°íšŒ
            recent_data = self.db_manager.get_recent_data(24)
            
            if not recent_data:
                logging.info("ìƒì„±í•  ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(recent_data)
            
            # íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"scraping_report_{timestamp}.xlsx"
            
            # Excel íŒŒì¼ ì €ì¥
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # ì „ì²´ ë°ì´í„°
                df.to_excel(writer, sheet_name='ì „ì²´_ë°ì´í„°', index=False)
                
                # ì›¹ì‚¬ì´íŠ¸ë³„ í†µê³„
                website_stats = df.groupby('website_name').agg({
                    'id': 'count',
                    'scraped_at': ['min', 'max']
                }).round(2)
                website_stats.to_excel(writer, sheet_name='ì›¹ì‚¬ì´íŠ¸ë³„_í†µê³„')
                
                # ì‹œê°„ë³„ í†µê³„
                df['hour'] = pd.to_datetime(df['scraped_at']).dt.hour
                hourly_stats = df.groupby('hour')['id'].count()
                hourly_stats.to_excel(writer, sheet_name='ì‹œê°„ë³„_í†µê³„')
            
            logging.info(f"Excel ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {filename}")
            
            # ì´ë©”ì¼ë¡œ ë¦¬í¬íŠ¸ ì „ì†¡
            subject = f"ğŸ“Š ì¼ì¼ ìŠ¤í¬ë˜í•‘ ë¦¬í¬íŠ¸ - {datetime.now().strftime('%Y-%m-%d')}"
            body = f"""
            <html>
            <body>
                <h2>ì¼ì¼ ìŠ¤í¬ë˜í•‘ ë¦¬í¬íŠ¸</h2>
                <p><strong>ë³´ê³ ì„œ ìƒì„± ì‹œê°„:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p><strong>ìˆ˜ì§‘ëœ ë°ì´í„°:</strong> {len(recent_data)}ê±´</p>
                <p><strong>ëª¨ë‹ˆí„°ë§ ì›¹ì‚¬ì´íŠ¸:</strong> {len(df['website_name'].unique())}ê°œ</p>
                
                <h3>ì›¹ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘ í˜„í™©</h3>
                <table border="1" style="border-collapse: collapse;">
                    <tr><th>ì›¹ì‚¬ì´íŠ¸</th><th>ìˆ˜ì§‘ ê±´ìˆ˜</th></tr>
            """
            
            for website, count in df['website_name'].value_counts().items():
                body += f"<tr><td>{website}</td><td>{count}</td></tr>"
            
            body += """
                </table>
                <br>
                <p>ìƒì„¸í•œ ë°ì´í„°ëŠ” ì²¨ë¶€ëœ Excel íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.</p>
            </body>
            </html>
            """
            
            self.notification_manager.send_email(subject, body)
            
        except Exception as e:
            logging.error(f"Excel ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def setup_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        # ê¸°ë³¸ì ìœ¼ë¡œ 30ë¶„ë§ˆë‹¤ ì‹¤í–‰
        schedule.every(30).minutes.do(self.run_single_scrape)
        
        # ì„¤ì •ì— ë”°ë¥¸ ê°œë³„ ìŠ¤ì¼€ì¤„
        for website_config in self.config['websites']:
            if website_config.get('enabled', True) and 'schedule' in website_config:
                schedule_str = website_config['schedule']
                
                # ìŠ¤ì¼€ì¤„ í˜•ì‹ì— ë”°ë¥¸ ì²˜ë¦¬
                if schedule_str.startswith('*/'):
                    # */30 (30ë¶„ë§ˆë‹¤)
                    interval = int(schedule_str[2:])
                    schedule.every(interval).minutes.do(
                        lambda w=website_config: self.scrape_single_website(w)
                    )
                elif schedule_str == 'daily':
                    schedule.every().day.at("09:00").do(
                        lambda w=website_config: self.scrape_single_website(w)
                    )
                elif schedule_str == 'hourly':
                    schedule.every().hour.do(
                        lambda w=website_config: self.scrape_single_website(w)
                    )
        
        # ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± (ë§¤ì¼ ìì •)
        if self.config['export_settings']['excel_export']:
            schedule.every().day.at("00:00").do(self.generate_excel_report)
        
        logging.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ")
    
    def scrape_single_website(self, website_config: Dict[str, Any]):
        """ë‹¨ì¼ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ (ìŠ¤ì¼€ì¤„ëŸ¬ìš©)"""
        data = self.scrape_website(website_config)
        if data:
            self.process_scraped_data(data)
    
    def run_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
        self.setup_scheduler()
        logging.info("ğŸ•·ï¸ ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ìë™í™” ì‹œì‘")
        logging.info("ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ í™•ì¸
        except KeyboardInterrupt:
            logging.info("ì‚¬ìš©ìì— ì˜í•´ ì¢…ë£Œë¨")
        except Exception as e:
            logging.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ•·ï¸ Advanced Web Scraping Automation Tool v1.0.0")
    print("=" * 50)
    print()
    
    try:
        scraper = AdvancedWebScraper()
        
        print("ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ë‹¨ì¼ ì‹¤í–‰ (í•œ ë²ˆë§Œ ìŠ¤í¬ë˜í•‘)")
        print("2. ìë™í™” ëª¨ë“œ (24ì‹œê°„ ìë™ ì‹¤í–‰)")
        print("3. ì„¤ì • í™•ì¸")
        print("4. Excel ë¦¬í¬íŠ¸ ìƒì„±")
        
        choice = input("\nì„ íƒ (1-4): ").strip()
        
        if choice == "1":
            print("\në‹¨ì¼ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            scraper.run_single_scrape()
            print("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            
        elif choice == "2":
            print("\n24ì‹œê°„ ìë™í™” ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            scraper.run_scheduler()
            
        elif choice == "3":
            print("\ní˜„ì¬ ì„¤ì •:")
            print(f"- ëª¨ë‹ˆí„°ë§ ì›¹ì‚¬ì´íŠ¸: {len(scraper.config['websites'])}ê°œ")
            print(f"- í™œì„±í™”ëœ ì›¹ì‚¬ì´íŠ¸: {len([w for w in scraper.config['websites'] if w.get('enabled', True)])}ê°œ")
            print(f"- ì´ë©”ì¼ ì•Œë¦¼: {'ON' if scraper.config['notifications']['email']['enabled'] else 'OFF'}")
            print(f"- ì›¹í›… ì•Œë¦¼: {'ON' if scraper.config['notifications']['webhook']['enabled'] else 'OFF'}")
            print(f"- Excel ë‚´ë³´ë‚´ê¸°: {'ON' if scraper.config['export_settings']['excel_export'] else 'OFF'}")
            
        elif choice == "4":
            print("\nExcel ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            scraper.generate_excel_report()
            print("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
            
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    except Exception as e:
        logging.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()